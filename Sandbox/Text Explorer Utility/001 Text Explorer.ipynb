{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7568babd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T21:27:51.167282Z",
     "start_time": "2022-04-13T21:27:47.334706Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca59e3",
   "metadata": {},
   "source": [
    "# Language Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab4738f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T21:41:34.219073Z",
     "start_time": "2022-04-13T21:41:30.721193Z"
    }
   },
   "outputs": [],
   "source": [
    "english = pd.read_html('https://github.com/igorbrigadir/stopwords/blob/master/en/terrier.txt')[0][1].to_list()\n",
    "spanish = pd.read_html('https://github.com/stopwords-iso/stopwords-es/blob/master/stopwords-es.txt')[0][1].to_list()\n",
    "russian = pd.read_html('https://github.com/stopwords-iso/stopwords-ru/blob/master/stopwords-ru.txt')[0][1].to_list()\n",
    "vietnamese = pd.read_html('https://github.com/stopwords/vietnamese-stopwords/blob/master/vietnamese-stopwords.txt')[0][1].to_list()\n",
    "korean = pd.read_html('https://gist.github.com/spikeekips/40eea22ef4a89f629abd87eed535ac6a')[0][1].to_list()\n",
    "mandarine = pd.read_html('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32439cc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T21:41:39.235447Z",
     "start_time": "2022-04-13T21:41:39.216288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가',\n",
       " '가까스로',\n",
       " '가령',\n",
       " '각',\n",
       " '각각',\n",
       " '각자',\n",
       " '각종',\n",
       " '갖고말하자면',\n",
       " '같다',\n",
       " '같이',\n",
       " '개의치않고',\n",
       " '거니와',\n",
       " '거바',\n",
       " '거의',\n",
       " '것',\n",
       " '것과 같이',\n",
       " '것들',\n",
       " '게다가',\n",
       " '게우다',\n",
       " '겨우',\n",
       " '견지에서',\n",
       " '결과에 이르다',\n",
       " '결국',\n",
       " '결론을 낼 수 있다',\n",
       " '겸사겸사',\n",
       " '고려하면',\n",
       " '고로',\n",
       " '곧',\n",
       " '공동으로',\n",
       " '과',\n",
       " '과연',\n",
       " '관계가 있다',\n",
       " '관계없이',\n",
       " '관련이 있다',\n",
       " '관하여',\n",
       " '관한',\n",
       " '관해서는',\n",
       " '구',\n",
       " '구체적으로',\n",
       " '구토하다',\n",
       " '그',\n",
       " '그들',\n",
       " '그때',\n",
       " '그래',\n",
       " '그래도',\n",
       " '그래서',\n",
       " '그러나',\n",
       " '그러니',\n",
       " '그러니까',\n",
       " '그러면',\n",
       " '그러므로',\n",
       " '그러한즉',\n",
       " '그런 까닭에',\n",
       " '그런데',\n",
       " '그런즉',\n",
       " '그럼',\n",
       " '그럼에도 불구하고',\n",
       " '그렇게 함으로써',\n",
       " '그렇지',\n",
       " '그렇지 않다면',\n",
       " '그렇지 않으면',\n",
       " '그렇지만',\n",
       " '그렇지않으면',\n",
       " '그리고',\n",
       " '그리하여',\n",
       " '그만이다',\n",
       " '그에 따르는',\n",
       " '그위에',\n",
       " '그저',\n",
       " '그중에서',\n",
       " '그치지 않다',\n",
       " '근거로',\n",
       " '근거하여',\n",
       " '기대여',\n",
       " '기점으로',\n",
       " '기준으로',\n",
       " '기타',\n",
       " '까닭으로',\n",
       " '까악',\n",
       " '까지',\n",
       " '까지 미치다',\n",
       " '까지도',\n",
       " '꽈당',\n",
       " '끙끙',\n",
       " '끼익',\n",
       " '나',\n",
       " '나머지는',\n",
       " '남들',\n",
       " '남짓',\n",
       " '너',\n",
       " '너희',\n",
       " '너희들',\n",
       " '네',\n",
       " '넷',\n",
       " '년',\n",
       " '논하지 않다',\n",
       " '놀라다',\n",
       " '누가 알겠는가',\n",
       " '누구',\n",
       " '다른',\n",
       " '다른 방면으로',\n",
       " '다만',\n",
       " '다섯',\n",
       " '다소',\n",
       " '다수',\n",
       " '다시 말하자면',\n",
       " '다시말하면',\n",
       " '다음',\n",
       " '다음에',\n",
       " '다음으로',\n",
       " '단지',\n",
       " '답다',\n",
       " '당신',\n",
       " '당장',\n",
       " '대로 하다',\n",
       " '대하면',\n",
       " '대하여',\n",
       " '대해 말하자면',\n",
       " '대해서',\n",
       " '댕그',\n",
       " '더구나',\n",
       " '더군다나',\n",
       " '더라도',\n",
       " '더불어',\n",
       " '더욱더',\n",
       " '더욱이는',\n",
       " '도달하다',\n",
       " '도착하다',\n",
       " '동시에',\n",
       " '동안',\n",
       " '된바에야',\n",
       " '된이상',\n",
       " '두번째로',\n",
       " '둘',\n",
       " '둥둥',\n",
       " '뒤따라',\n",
       " '뒤이어',\n",
       " '든간에',\n",
       " '들',\n",
       " '등',\n",
       " '등등',\n",
       " '딩동',\n",
       " '따라',\n",
       " '따라서',\n",
       " '따위',\n",
       " '따지지 않다',\n",
       " '딱',\n",
       " '때',\n",
       " '때가 되어',\n",
       " '때문에',\n",
       " '또',\n",
       " '또한',\n",
       " '뚝뚝',\n",
       " '라 해도',\n",
       " '령',\n",
       " '로',\n",
       " '로 인하여',\n",
       " '로부터',\n",
       " '로써',\n",
       " '륙',\n",
       " '를',\n",
       " '마음대로',\n",
       " '마저',\n",
       " '마저도',\n",
       " '마치',\n",
       " '막론하고',\n",
       " '만 못하다',\n",
       " '만약',\n",
       " '만약에',\n",
       " '만은 아니다',\n",
       " '만이 아니다',\n",
       " '만일',\n",
       " '만큼',\n",
       " '말하자면',\n",
       " '말할것도 없고',\n",
       " '매',\n",
       " '매번',\n",
       " '메쓰겁다',\n",
       " '몇',\n",
       " '모',\n",
       " '모두',\n",
       " '무렵',\n",
       " '무릎쓰고',\n",
       " '무슨',\n",
       " '무엇',\n",
       " '무엇때문에',\n",
       " '물론',\n",
       " '및',\n",
       " '바꾸어말하면',\n",
       " '바꾸어말하자면',\n",
       " '바꾸어서 말하면',\n",
       " '바꾸어서 한다면',\n",
       " '바꿔 말하면',\n",
       " '바로',\n",
       " '바와같이',\n",
       " '밖에 안된다',\n",
       " '반대로',\n",
       " '반대로 말하자면',\n",
       " '반드시',\n",
       " '버금',\n",
       " '보는데서',\n",
       " '보다더',\n",
       " '보드득',\n",
       " '본대로',\n",
       " '봐',\n",
       " '봐라',\n",
       " '부류의 사람들',\n",
       " '부터',\n",
       " '불구하고',\n",
       " '불문하고',\n",
       " '붕붕',\n",
       " '비걱거리다',\n",
       " '비교적',\n",
       " '비길수 없다',\n",
       " '비로소',\n",
       " '비록',\n",
       " '비슷하다',\n",
       " '비추어 보아',\n",
       " '비하면',\n",
       " '뿐만 아니라',\n",
       " '뿐만아니라',\n",
       " '뿐이다',\n",
       " '삐걱',\n",
       " '삐걱거리다',\n",
       " '사',\n",
       " '삼',\n",
       " '상대적으로 말하자면',\n",
       " '생각한대로',\n",
       " '설령',\n",
       " '설마',\n",
       " '설사',\n",
       " '셋',\n",
       " '소생',\n",
       " '소인',\n",
       " '솨',\n",
       " '쉿',\n",
       " '습니까',\n",
       " '습니다',\n",
       " '시각',\n",
       " '시간',\n",
       " '시작하여',\n",
       " '시초에',\n",
       " '시키다',\n",
       " '실로',\n",
       " '심지어',\n",
       " '아',\n",
       " '아니',\n",
       " '아니나다를가',\n",
       " '아니라면',\n",
       " '아니면',\n",
       " '아니었다면',\n",
       " '아래윗',\n",
       " '아무거나',\n",
       " '아무도',\n",
       " '아야',\n",
       " '아울러',\n",
       " '아이',\n",
       " '아이고',\n",
       " '아이구',\n",
       " '아이야',\n",
       " '아이쿠',\n",
       " '아하',\n",
       " '아홉',\n",
       " '안 그러면',\n",
       " '않기 위하여',\n",
       " '않기 위해서',\n",
       " '알 수 있다',\n",
       " '알았어',\n",
       " '앗',\n",
       " '앞에서',\n",
       " '앞의것',\n",
       " '야',\n",
       " '약간',\n",
       " '양자',\n",
       " '어',\n",
       " '어기여차',\n",
       " '어느',\n",
       " '어느 년도',\n",
       " '어느것',\n",
       " '어느곳',\n",
       " '어느때',\n",
       " '어느쪽',\n",
       " '어느해',\n",
       " '어디',\n",
       " '어때',\n",
       " '어떠한',\n",
       " '어떤',\n",
       " '어떤것',\n",
       " '어떤것들',\n",
       " '어떻게',\n",
       " '어떻해',\n",
       " '어이',\n",
       " '어째서',\n",
       " '어쨋든',\n",
       " '어쩔수 없다',\n",
       " '어찌',\n",
       " '어찌됏든',\n",
       " '어찌됏어',\n",
       " '어찌하든지',\n",
       " '어찌하여',\n",
       " '언제',\n",
       " '언젠가',\n",
       " '얼마',\n",
       " '얼마 안 되는 것',\n",
       " '얼마간',\n",
       " '얼마나',\n",
       " '얼마든지',\n",
       " '얼마만큼',\n",
       " '얼마큼',\n",
       " '엉엉',\n",
       " '에',\n",
       " '에 가서',\n",
       " '에 달려 있다',\n",
       " '에 대해',\n",
       " '에 있다',\n",
       " '에 한하다',\n",
       " '에게',\n",
       " '에서',\n",
       " '여',\n",
       " '여기',\n",
       " '여덟',\n",
       " '여러분',\n",
       " '여보시오',\n",
       " '여부',\n",
       " '여섯',\n",
       " '여전히',\n",
       " '여차',\n",
       " '연관되다',\n",
       " '연이서',\n",
       " '영',\n",
       " '영차',\n",
       " '옆사람',\n",
       " '예',\n",
       " '예를 들면',\n",
       " '예를 들자면',\n",
       " '예컨대',\n",
       " '예하면',\n",
       " '오',\n",
       " '오로지',\n",
       " '오르다',\n",
       " '오자마자',\n",
       " '오직',\n",
       " '오호',\n",
       " '오히려',\n",
       " '와',\n",
       " '와 같은 사람들',\n",
       " '와르르',\n",
       " '와아',\n",
       " '왜',\n",
       " '왜냐하면',\n",
       " '외에도',\n",
       " '요만큼',\n",
       " '요만한 것',\n",
       " '요만한걸',\n",
       " '요컨대',\n",
       " '우르르',\n",
       " '우리',\n",
       " '우리들',\n",
       " '우선',\n",
       " '우에 종합한것과같이',\n",
       " '운운',\n",
       " '월',\n",
       " '위에서 서술한바와같이',\n",
       " '위하여',\n",
       " '위해서',\n",
       " '윙윙',\n",
       " '육',\n",
       " '으로',\n",
       " '으로 인하여',\n",
       " '으로서',\n",
       " '으로써',\n",
       " '을',\n",
       " '응',\n",
       " '응당',\n",
       " '의',\n",
       " '의거하여',\n",
       " '의지하여',\n",
       " '의해',\n",
       " '의해되다',\n",
       " '의해서',\n",
       " '이',\n",
       " '이 되다',\n",
       " '이 때문에',\n",
       " '이 밖에',\n",
       " '이 외에',\n",
       " '이 정도의',\n",
       " '이것',\n",
       " '이곳',\n",
       " '이때',\n",
       " '이라면',\n",
       " '이래',\n",
       " '이러이러하다',\n",
       " '이러한',\n",
       " '이런',\n",
       " '이럴정도로',\n",
       " '이렇게 많은 것',\n",
       " '이렇게되면',\n",
       " '이렇게말하자면',\n",
       " '이렇구나',\n",
       " '이로 인하여',\n",
       " '이르기까지',\n",
       " '이리하여',\n",
       " '이만큼',\n",
       " '이번',\n",
       " '이봐',\n",
       " '이상',\n",
       " '이어서',\n",
       " '이었다',\n",
       " '이와 같다',\n",
       " '이와 같은',\n",
       " '이와 반대로',\n",
       " '이와같다면',\n",
       " '이외에도',\n",
       " '이용하여',\n",
       " '이유만으로',\n",
       " '이젠',\n",
       " '이지만',\n",
       " '이쪽',\n",
       " '이천구',\n",
       " '이천육',\n",
       " '이천칠',\n",
       " '이천팔',\n",
       " '인 듯하다',\n",
       " '인젠',\n",
       " '일',\n",
       " '일것이다',\n",
       " '일곱',\n",
       " '일단',\n",
       " '일때',\n",
       " '일반적으로',\n",
       " '일지라도',\n",
       " '임에 틀림없다',\n",
       " '입각하여',\n",
       " '입장에서',\n",
       " '잇따라',\n",
       " '있다',\n",
       " '자',\n",
       " '자기',\n",
       " '자기집',\n",
       " '자마자',\n",
       " '자신',\n",
       " '잠깐',\n",
       " '잠시',\n",
       " '저',\n",
       " '저것',\n",
       " '저것만큼',\n",
       " '저기',\n",
       " '저쪽',\n",
       " '저희',\n",
       " '전부',\n",
       " '전자',\n",
       " '전후',\n",
       " '점에서 보아',\n",
       " '정도에 이르다',\n",
       " '제',\n",
       " '제각기',\n",
       " '제외하고',\n",
       " '조금',\n",
       " '조차',\n",
       " '조차도',\n",
       " '졸졸',\n",
       " '좀',\n",
       " '좋아',\n",
       " '좍좍',\n",
       " '주룩주룩',\n",
       " '주저하지 않고',\n",
       " '줄은 몰랏다',\n",
       " '줄은모른다',\n",
       " '중에서',\n",
       " '중의하나',\n",
       " '즈음하여',\n",
       " '즉',\n",
       " '즉시',\n",
       " '지든지',\n",
       " '지만',\n",
       " '지말고',\n",
       " '진짜로',\n",
       " '쪽으로',\n",
       " '차라리',\n",
       " '참',\n",
       " '참나',\n",
       " '첫번째로',\n",
       " '쳇',\n",
       " '총적으로',\n",
       " '총적으로 말하면',\n",
       " '총적으로 보면',\n",
       " '칠',\n",
       " '콸콸',\n",
       " '쾅쾅',\n",
       " '쿵',\n",
       " '타다',\n",
       " '타인',\n",
       " '탕탕',\n",
       " '토하다',\n",
       " '통하여',\n",
       " '툭',\n",
       " '퉤',\n",
       " '틈타',\n",
       " '팍',\n",
       " '팔',\n",
       " '퍽',\n",
       " '펄렁',\n",
       " '하',\n",
       " '하게될것이다',\n",
       " '하게하다',\n",
       " '하겠는가',\n",
       " '하고 있다',\n",
       " '하고있었다',\n",
       " '하곤하였다',\n",
       " '하구나',\n",
       " '하기 때문에',\n",
       " '하기 위하여',\n",
       " '하기는한데',\n",
       " '하기만 하면',\n",
       " '하기보다는',\n",
       " '하기에',\n",
       " '하나',\n",
       " '하느니',\n",
       " '하는 김에',\n",
       " '하는 편이 낫다',\n",
       " '하는것도',\n",
       " '하는것만 못하다',\n",
       " '하는것이 낫다',\n",
       " '하는바',\n",
       " '하더라도',\n",
       " '하도다',\n",
       " '하도록시키다',\n",
       " '하도록하다',\n",
       " '하든지',\n",
       " '하려고하다',\n",
       " '하마터면',\n",
       " '하면 할수록',\n",
       " '하면된다',\n",
       " '하면서',\n",
       " '하물며',\n",
       " '하여금',\n",
       " '하여야',\n",
       " '하자마자',\n",
       " '하지 않는다면',\n",
       " '하지 않도록',\n",
       " '하지마',\n",
       " '하지마라',\n",
       " '하지만',\n",
       " '하하',\n",
       " '한 까닭에',\n",
       " '한 이유는',\n",
       " '한 후',\n",
       " '한다면',\n",
       " '한다면 몰라도',\n",
       " '한데',\n",
       " '한마디',\n",
       " '한적이있다',\n",
       " '한켠으로는',\n",
       " '한항목',\n",
       " '할 따름이다',\n",
       " '할 생각이다',\n",
       " '할 줄 안다',\n",
       " '할 지경이다',\n",
       " '할 힘이 있다',\n",
       " '할때',\n",
       " '할만하다',\n",
       " '할망정',\n",
       " '할뿐',\n",
       " '할수있다',\n",
       " '할수있어',\n",
       " '할줄알다',\n",
       " '할지라도',\n",
       " '할지언정',\n",
       " '함께',\n",
       " '해도된다',\n",
       " '해도좋다',\n",
       " '해봐요',\n",
       " '해서는 안된다',\n",
       " '해야한다',\n",
       " '해요',\n",
       " '했어요',\n",
       " '향하다',\n",
       " '향하여',\n",
       " '향해서',\n",
       " '허',\n",
       " '허걱',\n",
       " '허허',\n",
       " '헉',\n",
       " '헉헉',\n",
       " '헐떡헐떡',\n",
       " '형식으로 쓰여',\n",
       " '혹시',\n",
       " '혹은',\n",
       " '혼자',\n",
       " '훨씬',\n",
       " '휘익',\n",
       " '휴',\n",
       " '흐흐',\n",
       " '흥',\n",
       " '힘입어']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "938d444b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T21:31:52.566322Z",
     "start_time": "2022-04-13T21:31:52.560180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ac0e2",
   "metadata": {},
   "source": [
    "## Obtain Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c630c90a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T21:43:24.495871Z",
     "start_time": "2022-02-10T21:43:24.240893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train = fetch_20newsgroups()\n",
    "twenty_train.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69839aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T22:39:17.329476Z",
     "start_time": "2022-02-10T22:39:17.315227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\n",
      "Subject: Re: Rewording the Second Amendment (ideas)\n",
      "Organization: VTT\n",
      "Lines: 58\n",
      "\n",
      "In article <1r1eu1$4t@transfer.stratus.com> cdt@sw.stratus.com (C. D. Tavares) writes:\n",
      ">In article <1993Apr20.083057.16899@ousrvr.oulu.fi>, dfo@vttoulu.tko.vtt.fi (Foxvog Douglas) writes:\n",
      ">> In article <1qv87v$4j3@transfer.stratus.com> cdt@sw.stratus.com (C. D. Tavares) writes:\n",
      ">> >In article <C5n3GI.F8F@ulowell.ulowell.edu>, jrutledg@cs.ulowell.edu (John Lawrence Rutledge) writes:\n",
      ">\n",
      ">> >> The massive destructive power of many modern weapons, makes the\n",
      ">> >> cost of an accidental or crimial usage of these weapons to great.\n",
      ">> >> The weapons of mass destruction need to be in the control of\n",
      ">> >> the government only.  Individual access would result in the\n",
      ">> >> needless deaths of millions.  This makes the right of the people\n",
      ">> >> to keep and bear many modern weapons non-existant.\n",
      "\n",
      ">> >Thanks for stating where you're coming from.  Needless to say, I\n",
      ">> >disagree on every count.\n",
      "\n",
      ">> You believe that individuals should have the right to own weapons of\n",
      ">> mass destruction?  I find it hard to believe that you would support a \n",
      ">> neighbor's right to keep nuclear weapons, biological weapons, and nerve\n",
      ">> gas on his/her property.  \n",
      "\n",
      ">> If we cannot even agree on keeping weapons of mass destruction out of\n",
      ">> the hands of individuals, can there be any hope for us?\n",
      "\n",
      ">I don't sign any blank checks.\n",
      "\n",
      "Of course.  The term must be rigidly defined in any bill.\n",
      "\n",
      ">When Doug Foxvog says \"weapons of mass destruction,\" he means CBW and\n",
      ">nukes.  When Sarah Brady says \"weapons of mass destruction\" she means\n",
      ">Street Sweeper shotguns and semi-automatic SKS rifles.  \n",
      "\n",
      "I doubt she uses this term for that.  You are using a quote allegedly\n",
      "from her, can you back it up?\n",
      "\n",
      ">When John\n",
      ">Lawrence Rutledge says \"weapons of mass destruction,\" and then immediately\n",
      ">follows it with:\n",
      "\n",
      ">>> The US has thousands of people killed each year by handguns,\n",
      ">>> this number can easily be reduced by putting reasonable restrictions\n",
      ">>> on them.\n",
      "\n",
      ">...what does Rutledge mean by the term?\n",
      "\n",
      "I read the article as presenting first an argument about weapons of mass\n",
      "destruction (as commonly understood) and then switching to other topics.\n",
      "The first point evidently was to show that not all weapons should be\n",
      "allowed, and then the later analysis was, given this understanding, to\n",
      "consider another class.\n",
      "\n",
      ">cdt@rocket.sw.stratus.com   --If you believe that I speak for my company,\n",
      ">OR cdt@vos.stratus.com        write today for my special Investors' Packet...\n",
      "\n",
      "\n",
      "\n",
      "-- \n",
      "doug foxvog\n",
      "douglas.foxvog@vtt.fi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aaa4dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T21:43:29.016313Z",
     "start_time": "2022-02-10T21:43:29.010354Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(twenty_train['filenames']))\n",
    "print(twenty_train.target_names)\n",
    "print(twenty_train.DESCR)                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0fa86b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T16:52:47.684844Z",
     "start_time": "2021-12-15T16:52:47.675866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad701c7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T21:43:56.967181Z",
     "start_time": "2022-02-10T21:43:56.673796Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bca8e64fdb8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nSubject:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.iloc[2:3,0][2].split('\\nSubject:')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03e0cfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-10T21:43:57.712864Z",
     "start_time": "2022-02-10T21:43:57.697886Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-82a1b8f3ae39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain_body_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nLines: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_body_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "main_body_str = df.iloc[2:3,0][2].split('\\nLines: ')[1].split('\\n', maxsplit=1)[1]\n",
    "print(main_body_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a9c75c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T14:51:06.007001Z",
     "start_time": "2021-12-08T14:51:05.987674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\n",
      "Subject: help: Splitting a trimming region along a mesh \n",
      "Organization: University Of Kentucky, Dept. of Math Sciences\n",
      "Lines: 28\n",
      "\n",
      "\n",
      "\n",
      "\tHi,\n",
      "\n",
      "\tI have a problem, I hope some of the 'gurus' can help me solve.\n",
      "\n",
      "\tBackground of the problem:\n",
      "\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n",
      "\tmapping of a 3d Bezier patch into 2d. The area in this domain\n",
      "\twhich is inside a trimming loop had to be rendered. The trimming\n",
      "\tloop is a set of 2d Bezier curve segments.\n",
      "\tFor the sake of notation: the mesh is made up of cells.\n",
      "\n",
      "\tMy problem is this :\n",
      "\tThe trimming area has to be split up into individual smaller\n",
      "\tcells bounded by the trimming curve segments. If a cell\n",
      "\tis wholly inside the area...then it is output as a whole ,\n",
      "\telse it is trivially rejected. \n",
      "\n",
      "\tDoes any body know how thiss can be done, or is there any algo. \n",
      "\tsomewhere for doing this.\n",
      "\n",
      "\tAny help would be appreciated.\n",
      "\n",
      "\tThanks, \n",
      "\tAni.\n",
      "-- \n",
      "To get irritated is human, to stay cool, divine.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1:2,0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80a89027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T20:05:51.657628Z",
     "start_time": "2021-12-07T20:05:51.652553Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(twenty_train.data)\n",
    "row1 = df.iloc[0:1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "925ba6ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T14:47:46.290203Z",
     "start_time": "2021-12-08T14:47:46.258454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca5c5a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T20:08:55.323005Z",
     "start_time": "2021-12-07T20:08:55.308249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(row1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24f18fd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T20:08:27.595668Z",
     "start_time": "2021-12-07T20:08:27.586879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(row1)):\n",
    "    \n",
    "    print(row1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90f83ea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T20:07:42.231262Z",
     "start_time": "2021-12-07T20:07:42.220424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    From: sd345@city.ac.uk (Michael Collier)\\nSubj...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(row1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da39b00c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T18:26:56.093181Z",
     "start_time": "2021-12-07T18:26:56.076685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d95929f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T18:28:18.328440Z",
     "start_time": "2021-12-07T18:28:18.321282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n"
     ]
    }
   ],
   "source": [
    "print(len(twenty_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "824e787f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T18:28:33.839963Z",
     "start_time": "2021-12-07T18:28:33.825462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n"
     ]
    }
   ],
   "source": [
    "print(len(twenty_train.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea16a50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T18:39:33.200928Z",
     "start_time": "2021-12-07T18:39:33.185184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef906c31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T18:41:47.836624Z",
     "start_time": "2021-12-07T18:41:47.825424Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f741165e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T18:41:53.824732Z",
     "start_time": "2021-12-07T18:41:53.809596Z"
    }
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0346b47e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T18:42:00.884217Z",
     "start_time": "2021-12-07T18:42:00.328594Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_counts = count_vect.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "976c4afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T18:42:36.918336Z",
     "start_time": "2021-12-07T18:42:36.905149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4eeb488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T18:42:42.409557Z",
     "start_time": "2021-12-07T18:42:42.401486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2257x35788 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 365886 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43cc6632",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-07T18:42:53.077741Z",
     "start_time": "2021-12-07T18:42:53.072370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7155b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
